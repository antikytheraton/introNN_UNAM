{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron (XOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]: -0.0010968634060864457 -> 0\n",
      "[0 1]: 0.4362722541879846 -> 1\n",
      "[1 0]: 0.554984135943851 -> 1\n",
      "[1 1]: 0.992353253537922 -> 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5/8DELzzOtYuNRoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1855775b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "unit_step = lambda x: 0 if x < 0 else 1\n",
    "\n",
    "training_data = [\n",
    "    (array([0,0,1]), 0),\n",
    "    (array([0,1,1]), 1),\n",
    "    (array([1,0,1]), 1),\n",
    "    (array([1,1,1]), 1), ]\n",
    "\n",
    "w = random.rand(3)\n",
    "\n",
    "errors = []\n",
    "eta = 0.2 \n",
    "n = 50\n",
    "\n",
    "for i in range(n):\n",
    "    x, expected = choice(training_data)\n",
    "    result = dot(w, x)\n",
    "    error = expected - unit_step(result)\n",
    "    errors.append(error)\n",
    "    w += eta * error * x\n",
    "\n",
    "for x, _ in training_data:\n",
    "    result = dot(x, w)\n",
    "    print(\"{}: {} -> {}\".format(x[:2], result, unit_step(result)))\n",
    "    \n",
    "from pylab import plot, ylim, show\n",
    "ylim([-1,1]) \n",
    "plot(errors)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Program\n",
      "(1.0, 1.0, 0.014929208005738348, 0.0, 0.000222881251678602)\n",
      "(0.0, 1.0, 0.98572950473676912, 1.0, 0.00020364703505789487)\n",
      "(1.0, 0.0, 0.98562503368714638, 1.0, 0.00020663965649567642)\n",
      "(0.0, 0.0, 0.016607849913409585, 0.0, 0.0002758206787463388)\n",
      "---------------Net weights-------------\n",
      "[[ 0.          0.          5.75231929 -6.31595212  0.        ]\n",
      " [ 0.          0.         -5.97540997  6.18899346  0.        ]\n",
      " [ 0.          0.          0.          1.93019719  9.6814855 ]\n",
      " [ 0.          0.          0.          0.          9.57128428]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "[ 0.          0.          3.1933078   3.44466182  4.75885176]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "# note that this only works for a single layer of depth\n",
    "INPUT_NODES = 2\n",
    "OUTPUT_NODES = 1\n",
    "HIDDEN_NODES = 2\n",
    "\n",
    "# 15000 iterations is a good point for playing with learning rate\n",
    "MAX_ITERATIONS = 150000\n",
    "\n",
    "# setting this too low makes everything change very slowly, but too high\n",
    "# makes it jump at each and every example and oscillate. I found .5 to be good\n",
    "LEARNING_RATE = .2\n",
    "\n",
    "print(\"Neural Network Program\")\n",
    "\n",
    "class network:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.total_nodes = input_nodes + hidden_nodes + output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # set up the arrays\n",
    "        self.values = numpy.zeros(self.total_nodes)\n",
    "        self.expectedValues = numpy.zeros(self.total_nodes)\n",
    "        self.thresholds = numpy.zeros(self.total_nodes)\n",
    "\n",
    "        # the weight matrix is always square\n",
    "        self.weights = numpy.zeros((self.total_nodes, self.total_nodes))\n",
    "\n",
    "        # set random seed! this is so we can experiment consistently\n",
    "        random.seed(10000)\n",
    "\n",
    "        # set initial random values for weights and thresholds\n",
    "        # this is a strictly upper triangular matrix as there is no feedback\n",
    "        # loop and there inputs do not affect other inputs\n",
    "        for i in range(self.input_nodes, self.total_nodes):\n",
    "            self.thresholds[i] = random.random() / random.random()\n",
    "            for j in range(i + 1, self.total_nodes):\n",
    "                self.weights[i][j] = random.random() * 2\n",
    "\n",
    "    def process(self):\n",
    "        # update the hidden nodes\n",
    "        for i in range(self.input_nodes, self.input_nodes + self.hidden_nodes):\n",
    "            # sum weighted input nodes for each hidden node, compare threshold, apply sigmoid\n",
    "            W_i = 0.0\n",
    "            for j in range(self.input_nodes):\n",
    "                W_i += self.weights[j][i] * self.values[j]\n",
    "            W_i -= self.thresholds[i]\n",
    "            self.values[i] = 1 / (1 + math.exp(-W_i))\n",
    "\n",
    "        # update the output nodes\n",
    "        for i in range(self.input_nodes + self.hidden_nodes, self.total_nodes):\n",
    "            # sum weighted hidden nodes for each output node, compare threshold, apply sigmoid\n",
    "            W_i = 0.0\n",
    "            for j in range(self.input_nodes, self.input_nodes + self.hidden_nodes):\n",
    "                W_i += self.weights[j][i] * self.values[j]\n",
    "            W_i -= self.thresholds[i]\n",
    "            self.values[i] = 1 / (1 + math.exp(-W_i))\n",
    "\n",
    "    def processErrors(self):\n",
    "        sumOfSquaredErrors = 0.0\n",
    "\n",
    "        # we only look at the output nodes for error calculation\n",
    "        for i in range(self.input_nodes + self.hidden_nodes, self.total_nodes):\n",
    "            error = self.expectedValues[i] - self.values[i]\n",
    "            #print error\n",
    "            sumOfSquaredErrors += math.pow(error, 2)\n",
    "            outputErrorGradient = self.values[i] * (1 - self.values[i]) * error\n",
    "            #print outputErrorGradient\n",
    "\n",
    "            # now update the weights and thresholds\n",
    "            for j in range(self.input_nodes, self.input_nodes + self.hidden_nodes):\n",
    "                # first update for the hidden nodes to output nodes (1 layer)\n",
    "                delta = self.learning_rate * self.values[j] * outputErrorGradient\n",
    "                #print delta\n",
    "                self.weights[j][i] += delta\n",
    "                hiddenErrorGradient = self.values[j] * (1 - self.values[j]) * outputErrorGradient * self.weights[j][i]\n",
    "\n",
    "                # and then update for the input nodes to hidden nodes\n",
    "                for k in range(self.input_nodes):\n",
    "                    delta = self.learning_rate * self.values[k] * hiddenErrorGradient\n",
    "                    self.weights[k][j] += delta\n",
    "\n",
    "                # update the thresholds for the hidden nodes\n",
    "                delta = self.learning_rate * -1 * hiddenErrorGradient\n",
    "                #print delta\n",
    "                self.thresholds[j] += delta\n",
    "\n",
    "            # update the thresholds for the output node(s)\n",
    "            delta = self.learning_rate * -1 * outputErrorGradient\n",
    "            self.thresholds[i] += delta\n",
    "        return sumOfSquaredErrors\n",
    "\n",
    "class sampleMaker:\n",
    "    def __init__(self, network):\n",
    "        self.counter = 0\n",
    "        self.network = network\n",
    "\n",
    "    def setXor(self, x):\n",
    "        if x == 0:\n",
    "            self.network.values[0] = 1\n",
    "            self.network.values[1] = 1\n",
    "            self.network.expectedValues[4] = 0\n",
    "        elif x == 1:\n",
    "            self.network.values[0] = 0\n",
    "            self.network.values[1] = 1\n",
    "            self.network.expectedValues[4] = 1\n",
    "        elif x == 2:\n",
    "            self.network.values[0] = 1\n",
    "            self.network.values[1] = 0\n",
    "            self.network.expectedValues[4] = 1\n",
    "        else:\n",
    "            self.network.values[0] = 0\n",
    "            self.network.values[1] = 0\n",
    "            self.network.expectedValues[4] = 0\n",
    "\n",
    "    def setNextTrainingData(self):\n",
    "        self.setXor(self.counter % 4)\n",
    "        self.counter += 1\n",
    "\n",
    "# start of main program loop, initialize classes\n",
    "net = network(INPUT_NODES, HIDDEN_NODES, OUTPUT_NODES, LEARNING_RATE)\n",
    "samples = sampleMaker(net)\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    samples.setNextTrainingData()\n",
    "    net.process()\n",
    "    error = net.processErrors()\n",
    "\n",
    "    # prove that we got the right answers(ish)!\n",
    "    if i > (MAX_ITERATIONS - 5):\n",
    "        output = (net.values[0], net.values[1], net.values[4], net.expectedValues[4], error)\n",
    "        print(output)\n",
    "\n",
    "# display final parameters\n",
    "print(\"---------------Net weights-------------\")\n",
    "print(net.weights)\n",
    "print(net.thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0\n",
      "Inference  [[ 0.47944266]\n",
      " [ 0.93536353]\n",
      " [ 0.91219008]\n",
      " [ 0.98289716]]\n",
      "Cost  1.21381\n",
      "Batch  5000\n",
      "Inference  [[ 0.00189799]\n",
      " [ 0.97749209]\n",
      " [ 0.87864375]\n",
      " [ 0.09099805]]\n",
      "Cost  0.198023\n",
      "Batch  10000\n",
      "Inference  [[ 0.00159177]\n",
      " [ 0.97771674]\n",
      " [ 0.87882936]\n",
      " [ 0.09085816]]\n",
      "Cost  0.198015\n",
      "Batch  15000\n",
      "Inference  [[ 0.00156406]\n",
      " [ 0.97773504]\n",
      " [ 0.87884521]\n",
      " [ 0.09084859]]\n",
      "Cost  0.198015\n",
      "Batch  20000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  25000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  30000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  35000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  40000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  45000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  50000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  55000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  60000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  65000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  70000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  75000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  80000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  85000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  90000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  95000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  100000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  105000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  110000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  115000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  120000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  125000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  130000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  135000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  140000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  145000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  150000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  155000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  160000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  165000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  170000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  175000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  180000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  185000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  190000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n",
      "Batch  195000\n",
      "Inference  [[ 0.00156289]\n",
      " [ 0.97773468]\n",
      " [ 0.87884521]\n",
      " [ 0.09084916]]\n",
      "Cost  0.198015\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "### Feed forward three layer, Artificial Neural Network in Google Tensor Flow ###\n",
    "### Illustrates the learning of XOR logic with two activation functions       ###\n",
    "### \t\t\t\t\t\t\t\t\t      ###\t\n",
    "### Shows the ability to cope with noisy data and still learn patterns        ###\n",
    "### Usage: {Activation: Sigmoidal, Cost: [ACE, MSE]}                          ### \n",
    "###        {Activation: Tanh, Cost: [MSE]}                                    ### \n",
    "### \t\t\t\t\t\t\t\t\t      ### \n",
    "### Copyright Brett 2016                                                      ### \n",
    "#################################################################################\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def rand01(digit):\n",
    "    # Add some random noise to bits, but keep always between 0 and 1\n",
    "    s = abs(np.random.normal(0.0, 0.05))\n",
    "    if digit == 0:\n",
    "        noise = digit + s\n",
    "    else:\n",
    "        noise = digit - s\n",
    "    return noise\n",
    "\n",
    "### Training Examples\n",
    "### All combinations of XOR\n",
    "\n",
    "X = [[0, 0],[0, 1],[1, 0],[1, 1]]\n",
    "Y = [[0], [1], [1], [0]]\n",
    "\n",
    "# Add some random noise to our inputs. Useful if we use the tanh activiation function\n",
    "\n",
    "add_noise = np.vectorize(rand01)  \n",
    "X = add_noise(X)\n",
    "Y = add_noise(Y)\n",
    "\n",
    "# Neural Network Parameters\n",
    "\n",
    "N_STEPS = 200000\n",
    "N_EPOCH = 5000\n",
    "N_TRAINING = len(X)\n",
    "\n",
    "N_INPUT_NODES = 2\n",
    "N_HIDDEN_NODES = 5\n",
    "N_OUTPUT_NODES  = 1\n",
    "ACTIVATION = 'tanh' # sigmoid or tanh\n",
    "COST = 'ACE' # MSE or ACE\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ##############################################################################\n",
    "    ### Create placeholders for variables and define Neural Network structure  ###\n",
    "    ### Feed forward 3 layer, Neural Network.                                  ###\n",
    "    \n",
    "\n",
    "\n",
    "    x_ = tf.placeholder(tf.float32, shape=[N_TRAINING, N_INPUT_NODES], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, shape=[N_TRAINING, N_OUTPUT_NODES], name=\"y-input\")\n",
    "\n",
    "    theta1 = tf.Variable(tf.random_uniform([N_INPUT_NODES,N_HIDDEN_NODES], -1, 1), name=\"theta1\")\n",
    "    theta2 = tf.Variable(tf.random_uniform([N_HIDDEN_NODES,N_OUTPUT_NODES], -1, 1), name=\"theta2\")\n",
    "\n",
    "    bias1 = tf.Variable(tf.zeros([N_HIDDEN_NODES]), name=\"bias1\")\n",
    "    bias2 = tf.Variable(tf.zeros([N_OUTPUT_NODES]), name=\"bias2\")\n",
    "\n",
    "\n",
    "    if ACTIVATION == 'sigmoid':\n",
    "\n",
    "        ### Use a sigmoidal activation function ###\n",
    "\n",
    "        layer1 = tf.sigmoid(tf.matmul(x_, theta1) + bias1)\n",
    "        output = tf.sigmoid(tf.matmul(layer1, theta2) + bias2)\n",
    "\n",
    "    else:\n",
    "        ### Use tanh activation function ###\n",
    "\n",
    "        layer1 = tf.tanh(tf.matmul(x_, theta1) + bias1)\n",
    "        output = tf.tanh(tf.matmul(layer1, theta2) + bias2)\n",
    "    \n",
    "        output = tf.add(output, 1)\n",
    "        output = tf.multiply(output, 0.5)\n",
    "\n",
    "    \n",
    "    if COST == \"MSE\":\n",
    "\n",
    "        # Mean Squared Estimate - the simplist cost function (MSE)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.square(Y - output)) \n",
    "        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cost)\n",
    "    \n",
    "    else:\n",
    "        # Average Cross Entropy - better behaviour and learning rate\n",
    "\n",
    "        \n",
    "        cost = - tf.reduce_mean( (y_ * tf.log(output)) + (1 - y_) * tf.log(1.0 - output)  )\n",
    "        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "    # init = tf.initialize_all_variables()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    for i in range(N_STEPS):\n",
    "        sess.run(train_step, feed_dict={x_: X, y_: Y})\n",
    "        if i % N_EPOCH == 0:\n",
    "            print('Batch ', i)\n",
    "            print('Inference ', sess.run(output, feed_dict={x_: X, y_: Y}))\n",
    "            print('Cost ', sess.run(cost, feed_dict={x_: X, y_: Y}))\n",
    "            #print('op: ', sess.run(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
